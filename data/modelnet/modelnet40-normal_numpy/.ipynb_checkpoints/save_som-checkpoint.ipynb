{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numbers\n",
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "import struct\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "\n",
    "from util import som\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib qt5 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read numpy array data and label from h5_filename\n",
    "def load_h5(h5_filename):\n",
    "    f = h5py.File(h5_filename)\n",
    "    data = f['data'][:]\n",
    "    label = f['label'][:]\n",
    "    return (data, label)\n",
    "\n",
    "def make_dataset_modelnet40(root, is_train):\n",
    "    dataset = []\n",
    "\n",
    "    # get h5 file list\n",
    "    if True==is_train:\n",
    "        f = open(root+'train_files.txt', 'r')\n",
    "    else:\n",
    "        f = open(root+'test_files.txt', 'r')\n",
    "    file_names = [s.rstrip() for s in f.readlines()]\n",
    "    f.close()\n",
    "\n",
    "    for file in file_names:\n",
    "        data, label = load_h5(file)\n",
    "        for i in range(data.shape[0]):\n",
    "            item = (data[i], label[i,0])\n",
    "            dataset.append(item)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def som_saver(dataset, node_num, is_train):\n",
    "    rows = int(math.sqrt(node_num))\n",
    "    cols = rows\n",
    "    som_builder = som.SOM(rows, cols, dim=3, gpu_ids=True)\n",
    "    \n",
    "    node_tensor = torch.FloatTensor(len(dataset), 3, som_builder.node_num).zero_()\n",
    "    for i, item in enumerate(dataset):\n",
    "        pc_np, label = item\n",
    "        pc = torch.from_numpy(pc_np.transpose().astype(np.float32)).cuda()  # 3xN tensor\n",
    "        som_builder.optimize(pc)\n",
    "        node_tensor[i].copy_(som_builder.node)\n",
    "        \n",
    "        if i%100==0:\n",
    "            print(i)\n",
    "        \n",
    "    node_np = np.transpose(node_tensor.cpu().numpy(), axes=(0,2,1))  # BxNx3\n",
    "    if is_train:\n",
    "        np.save('data/som_train_%d.npy' % node_num, node_np)\n",
    "    else:\n",
    "        np.save('data/som_test_%d.npy' % node_num, node_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "node_num = 64\n",
    "is_train = False\n",
    "dataset = make_dataset_modelnet40('/ssd/dataset/modelnet40_ply_hdf5_2048/', is_train)\n",
    "som_saver(dataset, node_num, is_train)\n",
    "\n",
    "node_num = 64\n",
    "is_train = True\n",
    "dataset = make_dataset_modelnet40('/ssd/dataset/modelnet40_ply_hdf5_2048/', is_train)\n",
    "som_saver(dataset, node_num, is_train)\n",
    "\n",
    "node_num = 81\n",
    "is_train = False\n",
    "dataset = make_dataset_modelnet40('/ssd/dataset/modelnet40_ply_hdf5_2048/', is_train)\n",
    "som_saver(dataset, node_num, is_train)\n",
    "\n",
    "node_num = 81\n",
    "is_train = True\n",
    "dataset = make_dataset_modelnet40('/ssd/dataset/modelnet40_ply_hdf5_2048/', is_train)\n",
    "som_saver(dataset, node_num, is_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get som noded for all of modelnet 40\n",
    "def som_saver_modelnet40(root, rows, cols, gpu_ids):\n",
    "    som_builder = som.SOM(rows, cols, 3, gpu_ids)\n",
    "    \n",
    "    point_number = 4096\n",
    "    \n",
    "    f = open(os.path.join(root, 'filelist.txt'), 'r')\n",
    "    lines = [str.rstrip() for str in f.readlines()]\n",
    "    f.close()\n",
    "    \n",
    "    for i, name in enumerate(lines):\n",
    "        # locate the folder name\n",
    "        delimiter_idx = name.index('/')\n",
    "        folder = name[0:delimiter_idx]\n",
    "        file_name = name[delimiter_idx+1:-4]\n",
    "        \n",
    "        # load & random sample\n",
    "        pc_np = np.load(os.path.join(root, folder, file_name+'.npy'))[:,0:3]  # Nx6 -> Nx3\n",
    "        pc_np = pc_np[np.random.choice(pc_np.shape[0], point_number, replace=False), :]\n",
    "        \n",
    "        # som optimize\n",
    "        pc = torch.from_numpy(pc_np.transpose().astype(np.float32)).cuda()  # 3xN tensor\n",
    "        som_builder.optimize(pc)\n",
    "        som_node_np = som_builder.node.cpu().numpy().transpose().astype(np.float32)  # node_numx3\n",
    "        \n",
    "        # save to file\n",
    "        save_folder = '%dx%d_som_nodes' % (rows, cols)\n",
    "        if not os.path.isdir(os.path.join(root, save_folder, folder)):\n",
    "            os.mkdir(os.path.join(root, save_folder, folder))\n",
    "        np.save(os.path.join(root, save_folder, folder, file_name), som_node_np)\n",
    "        \n",
    "        print('%d, %s' % (i, file_name))\n",
    "        \n",
    "#         print(som_node_np)\n",
    "#         print(som_node_np.shape)\n",
    "        \n",
    "#         x_np = pc_np\n",
    "#         node_np = som_node_np\n",
    "#     #     print(node_np)\n",
    "#         fig = plt.figure()\n",
    "#         ax = Axes3D(fig)\n",
    "#         ax.scatter(x_np[:,0].tolist(), x_np[:,1].tolist(), x_np[:,2].tolist(), s=1)\n",
    "#         ax.scatter(node_np[:,0].tolist(), node_np[:,1].tolist(), node_np[:,2].tolist(), s=6, c='r')\n",
    "#         plt.show()\n",
    "        \n",
    "#         if i>10:\n",
    "#             break\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "som_saver_modelnet40('/ssd/LJX/dataset/modelnet40-normal_numpy', 11, 11, True)\n",
    "som_saver_modelnet40('/ssd/LJX/dataset/modelnet40-normal_numpy', 10, 10, True)\n",
    "som_saver_modelnet40('/ssd/LJX/dataset/modelnet40-normal_numpy', 6, 6, True)\n",
    "som_saver_modelnet40('/ssd/LJX/dataset/modelnet40-normal_numpy', 5, 5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_dataset_modelnet40(root, node_num, is_train):\n",
    "    dataset = []\n",
    "\n",
    "    # get h5 file list\n",
    "    if True==is_train:\n",
    "        f = open(root+'train_files.txt', 'r')\n",
    "        som_node = np.load('data/som_train_%d.npy' % node_num)\n",
    "    else:\n",
    "        f = open(root+'test_files.txt', 'r')\n",
    "        som_node = np.load('data/som_test_%d.npy' % node_num)\n",
    "    file_names = [s.rstrip() for s in f.readlines()]\n",
    "    f.close()\n",
    "\n",
    "    for file in file_names:\n",
    "        data, label = load_h5(file)\n",
    "        for i in range(data.shape[0]):\n",
    "            item = (data[i], label[i,0], som_node[i])\n",
    "            dataset.append(item)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "dataset = make_dataset_modelnet40('/ssd/dataset/modelnet40_ply_hdf5_2048/', 25, True)\n",
    "for i in range(len(dataset)):\n",
    "    pc_np, class_id, som_node_np = dataset[i]\n",
    "    \n",
    "    x_np = pc_np\n",
    "    node_np = som_node_np\n",
    "#     print(node_np)\n",
    "    fig = plt.figure()\n",
    "    ax = Axes3D(fig)\n",
    "    ax.scatter(x_np[:,0].tolist(), x_np[:,1].tolist(), x_np[:,2].tolist(), s=1)\n",
    "    ax.scatter(node_np[:,0].tolist(), node_np[:,1].tolist(), node_np[:,2].tolist(), s=6, c='r')\n",
    "    plt.show()\n",
    "    \n",
    "    if i>=10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 6)\n",
      "(49, 3)\n",
      "0\n",
      "(5000, 6)\n",
      "(49, 3)\n",
      "0\n",
      "(5000, 6)\n",
      "(49, 3)\n",
      "0\n",
      "(5000, 6)\n",
      "(49, 3)\n",
      "0\n",
      "(5000, 6)\n",
      "(49, 3)\n",
      "0\n",
      "(5000, 6)\n",
      "(49, 3)\n",
      "0\n",
      "(5000, 6)\n",
      "(49, 3)\n",
      "0\n",
      "(5000, 6)\n",
      "(49, 3)\n",
      "0\n",
      "(5000, 6)\n",
      "(49, 3)\n",
      "0\n",
      "(5000, 6)\n",
      "(49, 3)\n",
      "0\n",
      "(5000, 6)\n",
      "(49, 3)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "def make_dataset_modelnet40_10k(root, node_num, is_train):\n",
    "    dataset = []\n",
    "    rows = round(math.sqrt(node_num))\n",
    "    cols = rows\n",
    "    \n",
    "    f = open(os.path.join(root, 'modelnet40_shape_names.txt'))\n",
    "    shape_list = [str.rstrip() for str in f.readlines()]\n",
    "    f.close()\n",
    "    \n",
    "    if True==is_train:\n",
    "        f = open(os.path.join(root, 'modelnet40_train.txt'), 'r')\n",
    "        lines = [str.rstrip() for str in f.readlines()]\n",
    "        f.close()\n",
    "    else:\n",
    "        f = open(os.path.join(root, 'modelnet40_test.txt'), 'r')\n",
    "        lines = [str.rstrip() for str in f.readlines()]\n",
    "        f.close()\n",
    "    \n",
    "    for i, name in enumerate(lines):\n",
    "        # locate the folder name\n",
    "        folder = name[0:-5]\n",
    "        file_name = name\n",
    "        \n",
    "        # get the label\n",
    "        label = shape_list.index(folder)\n",
    "        \n",
    "        # som node locations\n",
    "        som_nodes_folder = '%dx%d_som_nodes' % (rows, cols)\n",
    "        \n",
    "        item = (os.path.join(root, folder, file_name+'.npy'), \n",
    "                os.path.join(root, som_nodes_folder, folder, file_name+'.npy'), \n",
    "                label)\n",
    "        dataset.append(item)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "dataset = make_dataset_modelnet40_10k('/ssd/dataset/modelnet40-normal_numpy/', 64, False)\n",
    "for i in range(len(dataset)):\n",
    "    pc_np_file, som_node_np_file, label = dataset[i]\n",
    "    \n",
    "    pc_np = np.load(pc_np_file)\n",
    "    pc_np = pc_np[np.random.choice(pc_np.shape[0], 5000, replace=False), :]\n",
    "    som_node_np = np.load(som_node_np_file)\n",
    "    \n",
    "    print(pc_np.shape)\n",
    "    print(som_node_np.shape)\n",
    "    print(label)\n",
    "    \n",
    "    x_np = pc_np\n",
    "    node_np = som_node_np\n",
    "#     print(node_np)\n",
    "    fig = plt.figure()\n",
    "    ax = Axes3D(fig)\n",
    "    ax.scatter(x_np[:,0].tolist(), x_np[:,1].tolist(), x_np[:,2].tolist(), s=1)\n",
    "    ax.scatter(node_np[:,0].tolist(), node_np[:,1].tolist(), node_np[:,2].tolist(), s=6, c='r')\n",
    "    plt.show()\n",
    "    \n",
    "    if i>=10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
